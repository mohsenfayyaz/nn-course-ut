{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Q3/pre_main_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "trainvalid_test = dataset.train_test_split(test_size=0.2)\n",
    "train_valid = trainvalid_test['train'].train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'validation': train_valid['test'],\n",
    "    'test': trainvalid_test['test'],\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"tweet\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"tweet\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plm = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "plm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, input_size=768, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, \n",
    "                                  hidden_size=384, num_layers=1, \n",
    "                                  bidirectional=True, batch_first=True).to(device)\n",
    "        net_list = [\n",
    "            torch.nn.Linear(768, 512),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(512, num_classes)        \n",
    "        ]\n",
    "        self.label_net = torch.nn.Sequential(*net_list).to(device)\n",
    "        self.training_criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=5e-4, weight_decay=0)\n",
    "        \n",
    "    def forward(self, plm_last_hidden_states):  # ~[8, 34, 768]\n",
    "        x, (hn, cn) = self.lstm(plm_last_hidden_states)\n",
    "        x = x[:, -1, :]  # Last LSTM\n",
    "        x = self.label_net(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, plm):\n",
    "        self.plm = plm\n",
    "        self.head = Head()\n",
    "        print(self.head)\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        plm.eval()\n",
    "        self.head.train()\n",
    "        for epoch in tqdm(range(5)):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items() if k not in [\"id\", \"tweet\"]}\n",
    "                outputs = self.plm(**batch, output_hidden_states=True, return_dict=True)\n",
    "                hidden_states = torch.stack([val.detach() for val in outputs.hidden_states])  # ~[13, 8, 34, 768]\n",
    "                last_hidden_states = hidden_states[-1].to(device)\n",
    "                output = self.head(last_hidden_states)\n",
    "#                 print(output)\n",
    "#                 print(batch[\"labels\"].float())\n",
    "                loss = self.head.training_criterion(output.to(device), torch.tensor(batch[\"labels\"], dtype=torch.long))\n",
    "                loss.backward()\n",
    "                self.head.optimizer.step()\n",
    "                self.head.optimizer.zero_grad()\n",
    "        #         progress_bar.update(1)\n",
    "\n",
    "trainer = Trainer(plm)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
